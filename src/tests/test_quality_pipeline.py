#!/usr/bin/env python3
import os
import sys
import asyncio
import argparse
import json
from dotenv import load_dotenv
from core.quality_pipeline import QualityPipeline
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('test_quality_pipeline')

# Load environment variables
load_dotenv()

async def test_generate_and_evaluate(subject, num_candidates=3, economy_mode=False):
    """
    Teste la g√©n√©ration et l'√©valuation de punchlines
    """
    logger.info(f"üß™ Test de g√©n√©ration et √©valuation pour le sujet: '{subject}'")
    logger.info(f"üîß Param√®tres: {num_candidates} candidats, mode √©conomie: {economy_mode}")
    
    # Initialiser la pipeline
    pipeline = QualityPipeline()
    
    # G√©n√©rer et √©valuer les punchlines
    evaluated_punchlines = await pipeline.generate_and_evaluate_punchlines(
        subject=subject,
        economy_mode=economy_mode,
        num_candidates=num_candidates
    )
    
    # Afficher les r√©sultats
    logger.info(f"‚úÖ {len(evaluated_punchlines)} punchlines g√©n√©r√©es et √©valu√©es")
    
    for i, punchline in enumerate(evaluated_punchlines):
        logger.info(f"\nüìù Punchline #{i+1}: {punchline['text']}")
        logger.info(f"üìä Score global: {punchline['overall_score']:.2f}")
        logger.info(f"üìä Scores d√©taill√©s:")
        for criterion, score in punchline['evaluation'].items():
            logger.info(f"  - {criterion}: {score:.2f}")
    
    return evaluated_punchlines

async def test_filter_quality(subject, threshold=0.7, num_candidates=3, economy_mode=False):
    """
    Teste le filtrage des punchlines par qualit√©
    """
    logger.info(f"üß™ Test de filtrage par qualit√© pour le sujet: '{subject}'")
    logger.info(f"üîß Param√®tres: seuil={threshold}, {num_candidates} candidats, mode √©conomie: {economy_mode}")
    
    # Initialiser la pipeline
    pipeline = QualityPipeline()
    
    # G√©n√©rer et √©valuer les punchlines
    evaluated_punchlines = await pipeline.generate_and_evaluate_punchlines(
        subject=subject,
        economy_mode=economy_mode,
        num_candidates=num_candidates
    )
    
    # Filtrer les punchlines par qualit√©
    quality_punchlines = await pipeline.filter_quality_punchlines(
        evaluated_punchlines=evaluated_punchlines,
        threshold=threshold
    )
    
    # Afficher les r√©sultats
    logger.info(f"‚úÖ {len(quality_punchlines)}/{len(evaluated_punchlines)} punchlines ont d√©pass√© le seuil de qualit√© ({threshold})")
    
    for i, punchline in enumerate(quality_punchlines):
        logger.info(f"\nüìù Punchline de qualit√© #{i+1}: {punchline['text']}")
        logger.info(f"üìä Score global: {punchline['overall_score']:.2f}")
    
    return quality_punchlines

async def test_get_best_punchline(subject, threshold=0.7, num_candidates=3, economy_mode=False):
    """
    Teste la s√©lection de la meilleure punchline
    """
    logger.info(f"üß™ Test de s√©lection de la meilleure punchline pour le sujet: '{subject}'")
    logger.info(f"üîß Param√®tres: seuil={threshold}, {num_candidates} candidats, mode √©conomie: {economy_mode}")
    
    # Initialiser la pipeline
    pipeline = QualityPipeline()
    
    # Obtenir la meilleure punchline
    best_text, best_metadata = await pipeline.get_best_punchline(
        subject=subject,
        economy_mode=economy_mode,
        threshold=threshold,
        num_candidates=num_candidates
    )
    
    # Afficher les r√©sultats
    logger.info(f"\nüèÜ Meilleure punchline: {best_text}")
    logger.info(f"üìä Score global: {best_metadata['overall_score']:.2f}")
    logger.info(f"üìä Scores d√©taill√©s:")
    for criterion, score in best_metadata['evaluation'].items():
        logger.info(f"  - {criterion}: {score:.2f}")
    
    return best_text, best_metadata

async def test_stats():
    """
    Teste la r√©cup√©ration des statistiques
    """
    logger.info(f"üß™ Test de r√©cup√©ration des statistiques")
    
    # Initialiser la pipeline
    pipeline = QualityPipeline()
    
    # R√©cup√©rer les statistiques
    stats = pipeline.get_evaluation_stats()
    
    # Afficher les r√©sultats
    logger.info(f"\nüìä Statistiques de la pipeline de qualit√©:")
    logger.info(f"Total des punchlines √©valu√©es: {stats['total_punchlines']}")
    logger.info(f"Punchlines s√©lectionn√©es: {stats['selected_punchlines']}")
    logger.info(f"Score moyen global: {stats['average_score']:.2f}")
    logger.info(f"Score moyen des punchlines s√©lectionn√©es: {stats['average_selected_score']:.2f}")
    logger.info(f"Scores moyens par crit√®re:")
    for criterion, score in stats['average_criteria'].items():
        logger.info(f"  - {criterion}: {score:.2f}")
    
    return stats

async def test_batch_subjects(subjects, threshold=0.7, num_candidates=3, economy_mode=False):
    """
    Teste la g√©n√©ration de punchlines pour plusieurs sujets
    """
    logger.info(f"üß™ Test de g√©n√©ration par lots pour {len(subjects)} sujets")
    logger.info(f"üîß Param√®tres: seuil={threshold}, {num_candidates} candidats, mode √©conomie: {economy_mode}")
    
    results = []
    
    # Initialiser la pipeline
    pipeline = QualityPipeline()
    
    for i, subject in enumerate(subjects):
        logger.info(f"\n[{i+1}/{len(subjects)}] ü§ñ Traitement du sujet: \"{subject}\"")
        
        # Obtenir la meilleure punchline
        best_text, best_metadata = await pipeline.get_best_punchline(
            subject=subject,
            economy_mode=economy_mode,
            threshold=threshold,
            num_candidates=num_candidates
        )
        
        # Afficher les r√©sultats
        logger.info(f"üèÜ Meilleure punchline: {best_text}")
        logger.info(f"üìä Score global: {best_metadata['overall_score']:.2f}")
        
        results.append({
            "subject": subject,
            "text": best_text,
            "score": best_metadata['overall_score'],
            "evaluation": best_metadata['evaluation']
        })
    
    # Afficher les statistiques
    await test_stats()
    
    return results

async def main():
    parser = argparse.ArgumentParser(description='Test de la pipeline de qualit√© pour les punchlines')
    parser.add_argument('-s', '--subject', type=str, help='Sujet pour la g√©n√©ration de punchlines')
    parser.add_argument('-b', '--batch', type=str, help='Fichier JSON contenant une liste de sujets')
    parser.add_argument('-n', '--num-candidates', type=int, default=3, help='Nombre de punchlines candidates √† g√©n√©rer')
    parser.add_argument('-t', '--threshold', type=float, default=0.7, help='Seuil de qualit√© (entre 0 et 1)')
    parser.add_argument('-e', '--economy', action='store_true', help='Activer le mode √©conomie de tokens')
    parser.add_argument('--test-all', action='store_true', help='Ex√©cuter tous les tests pour un sujet donn√©')
    parser.add_argument('--stats', action='store_true', help='Afficher les statistiques de la pipeline')
    
    args = parser.parse_args()
    
    if args.stats:
        await test_stats()
        return
    
    if args.batch:
        try:
            with open(args.batch, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            if 'sujets_clivants' in data:
                subjects = data['sujets_clivants']
            elif isinstance(data, list):
                subjects = data
            else:
                logger.error("‚ùå Format de fichier JSON invalide. Doit contenir une cl√© 'sujets_clivants' ou √™tre une liste.")
                return
            
            results = await test_batch_subjects(
                subjects=subjects[:5] if len(subjects) > 5 else subjects,  # Limiter √† 5 sujets par d√©faut
                threshold=args.threshold,
                num_candidates=args.num_candidates,
                economy_mode=args.economy
            )
            
            # Sauvegarder les r√©sultats dans un fichier JSON
            output_path = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
                'output',
                'reports',
                'test_results.json'
            )
            os.makedirs(os.path.dirname(output_path), exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"\nüíæ R√©sultats sauvegard√©s dans: {output_path}")
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du traitement du fichier batch: {str(e)}")
            return
    
    elif args.subject:
        if args.test_all:
            # Ex√©cuter tous les tests pour le sujet donn√©
            await test_generate_and_evaluate(
                subject=args.subject,
                num_candidates=args.num_candidates,
                economy_mode=args.economy
            )
            
            await test_filter_quality(
                subject=args.subject,
                threshold=args.threshold,
                num_candidates=args.num_candidates,
                economy_mode=args.economy
            )
            
            await test_get_best_punchline(
                subject=args.subject,
                threshold=args.threshold,
                num_candidates=args.num_candidates,
                economy_mode=args.economy
            )
            
            await test_stats()
        else:
            # Ex√©cuter uniquement le test de la meilleure punchline
            await test_get_best_punchline(
                subject=args.subject,
                threshold=args.threshold,
                num_candidates=args.num_candidates,
                economy_mode=args.economy
            )
    else:
        logger.error("‚ùå Veuillez sp√©cifier un sujet (-s) ou un fichier batch (-b)")
        parser.print_help()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Op√©ration annul√©e par l'utilisateur")
        sys.exit(0)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale: {str(e)}")
        sys.exit(1) 