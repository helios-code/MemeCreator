"""
DEPRECATED: This module is deprecated and will be removed in a future version.
Please use the new MVC structure instead:
- models/punchline_model.py for punchline data storage
- controllers/punchline_controller.py for punchline generation and evaluation logic
"""

import warnings

warnings.warn(
    "The QualityPipeline class is deprecated and will be removed in a future version. "
    "Please use PunchlineController from controllers/punchline_controller.py instead.",
    DeprecationWarning,
    stacklevel=2
)

import os
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from openai import OpenAI
from dotenv import load_dotenv
import sqlite3
from datetime import datetime
import re

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger('quality_pipeline')

# Load environment variables
load_dotenv()

class QualityPipeline:
    """
    Pipeline de qualit√© pour √©valuer et filtrer les punchlines g√©n√©r√©es
    """
    
    def __init__(self, db_path: str = None):
        """
        Initialise la pipeline de qualit√©
        
        Args:
            db_path: Chemin vers la base de donn√©es (par d√©faut: output/quality_data.db)
        """
        # Charger les variables d'environnement
        load_dotenv()
        
        # Configurer le logger
        self.logger = logging.getLogger('quality_pipeline')
        
        # Chemin de la base de donn√©es
        if db_path is None:
            self.db_path = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
                'data',
                'quality_data.db'
            )
        else:
            self.db_path = db_path
        
        self.api_key = os.getenv('OPENAI_API_KEY')
        self.client = OpenAI(api_key=self.api_key)
        
        # Seuils de qualit√© (configurables)
        self.quality_threshold = float(os.getenv('QUALITY_THRESHOLD', '0.7'))  # Seuil par d√©faut: 0.7
        self.num_candidates = int(os.getenv('NUM_PUNCHLINE_CANDIDATES', '3'))  # Nombre de punchlines √† g√©n√©rer
        
        self._init_database()
        
        logger.info(f"üîç Pipeline de qualit√© initialis√©e (seuil: {self.quality_threshold}, candidats: {self.num_candidates})")
    
    def _init_database(self):
        """Initialise la base de donn√©es SQLite pour stocker les √©valuations"""
        os.makedirs(os.path.dirname(self.db_path), exist_ok=True)
        
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Table pour les punchlines √©valu√©es
        cursor.execute('''
        CREATE TABLE IF NOT EXISTS punchlines (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            text TEXT NOT NULL,
            subject TEXT,
            timestamp DATETIME DEFAULT CURRENT_TIMESTAMP,
            originality REAL,
            humor REAL,
            relevance REAL,
            conciseness REAL,
            impact REAL,
            overall_score REAL,
            selected BOOLEAN DEFAULT 0,
            feedback TEXT
        )
        ''')
        
        conn.commit()
        conn.close()
    
    async def generate_and_evaluate_punchlines(
        self, 
        subject: str, 
        economy_mode: bool = False,
        num_candidates: Optional[int] = None
    ) -> List[Dict[str, Any]]:
        """
        G√©n√®re plusieurs punchlines candidates et les √©value
        
        Args:
            subject: Le sujet sur lequel g√©n√©rer des punchlines
            economy_mode: Utiliser le mode √©conomie de tokens
            num_candidates: Nombre de punchlines √† g√©n√©rer (utilise la valeur par d√©faut si None)
            
        Returns:
            Liste des punchlines √©valu√©es, tri√©es par score de qualit√© (meilleure en premier)
        """
        # Utiliser le nombre de candidats sp√©cifi√© ou la valeur par d√©faut
        n_candidates = num_candidates or self.num_candidates
        
        # G√©n√©rer plusieurs punchlines candidates
        candidates = await self._generate_candidate_punchlines(subject, n_candidates, economy_mode)
        
        # √âvaluer chaque punchline
        evaluated_punchlines = []
        for punchline in candidates:
            evaluation = await self._evaluate_punchline(subject, punchline)
            
            # Calculer le score global
            overall_score = self._calculate_overall_score(evaluation)
            
            # Cr√©er l'objet punchline √©valu√©e
            evaluated_punchline = {
                "text": punchline,
                "subject": subject,
                "evaluation": evaluation,
                "overall_score": overall_score
            }
            
            # Stocker dans la base de donn√©es
            self._store_evaluation(punchline, subject, evaluation, overall_score)
            
            evaluated_punchlines.append(evaluated_punchline)
        
        # Trier par score global (du plus √©lev√© au plus bas)
        evaluated_punchlines.sort(key=lambda x: x["overall_score"], reverse=True)
        
        return evaluated_punchlines
    
    async def filter_quality_punchlines(
        self, 
        evaluated_punchlines: List[Dict[str, Any]], 
        threshold: Optional[float] = None
    ) -> List[Dict[str, Any]]:
        """
        Filtre les punchlines en fonction du seuil de qualit√©
        
        Args:
            evaluated_punchlines: Liste des punchlines √©valu√©es
            threshold: Seuil de qualit√© (utilise la valeur par d√©faut si None)
            
        Returns:
            Liste des punchlines qui d√©passent le seuil de qualit√©
        """
        # Utiliser le seuil sp√©cifi√© ou la valeur par d√©faut
        quality_threshold = threshold or self.quality_threshold
        
        # Filtrer les punchlines qui d√©passent le seuil
        quality_punchlines = [p for p in evaluated_punchlines if p["overall_score"] >= quality_threshold]
        
        logger.info(f"üîç {len(quality_punchlines)}/{len(evaluated_punchlines)} punchlines ont d√©pass√© le seuil de qualit√© ({quality_threshold})")
        
        return quality_punchlines
    
    async def get_best_punchline(
        self, 
        subject: str, 
        economy_mode: bool = False,
        threshold: Optional[float] = None,
        num_candidates: Optional[int] = None
    ) -> Tuple[str, Dict[str, Any]]:
        """
        G√©n√®re et √©value plusieurs punchlines, puis retourne la meilleure
        
        Args:
            subject: Le sujet sur lequel g√©n√©rer des punchlines
            economy_mode: Utiliser le mode √©conomie de tokens
            threshold: Seuil de qualit√© (utilise la valeur par d√©faut si None)
            num_candidates: Nombre de punchlines √† g√©n√©rer (utilise la valeur par d√©faut si None)
            
        Returns:
            Tuple contenant la meilleure punchline et ses m√©tadonn√©es d'√©valuation
        """
        # G√©n√©rer et √©valuer les punchlines
        evaluated_punchlines = await self.generate_and_evaluate_punchlines(
            subject, 
            economy_mode,
            num_candidates
        )
        
        # Filtrer par qualit√©
        quality_punchlines = await self.filter_quality_punchlines(
            evaluated_punchlines,
            threshold
        )
        
        # Si aucune punchline ne d√©passe le seuil, prendre la meilleure disponible
        if not quality_punchlines and evaluated_punchlines:
            logger.warning("‚ö†Ô∏è Aucune punchline n'a d√©pass√© le seuil de qualit√©. Utilisation de la meilleure disponible.")
            best_punchline = evaluated_punchlines[0]
        elif quality_punchlines:
            # Prendre la meilleure punchline qui d√©passe le seuil
            best_punchline = quality_punchlines[0]
        else:
            # Cas improbable: aucune punchline g√©n√©r√©e
            raise ValueError("Aucune punchline n'a pu √™tre g√©n√©r√©e.")
        
        # Marquer cette punchline comme s√©lectionn√©e dans la base de donn√©es
        self._mark_as_selected(best_punchline["text"])
        
        return best_punchline["text"], best_punchline
    
    async def _generate_candidate_punchlines(
        self, 
        subject: str, 
        num_candidates: int,
        economy_mode: bool
    ) -> List[str]:
        """
        G√©n√®re plusieurs punchlines candidates pour un sujet donn√©
        
        Args:
            subject: Le sujet sur lequel g√©n√©rer des punchlines
            num_candidates: Nombre de punchlines √† g√©n√©rer
            economy_mode: Utiliser le mode √©conomie de tokens
            
        Returns:
            Liste des punchlines g√©n√©r√©es
        """
        try:
            # D√©terminer le mod√®le et les param√®tres en fonction du mode √©conomie
            if economy_mode:
                model = "gpt-3.5-turbo"
                system_content = """Tu es un g√©n√©rateur de punchlines ULTRA-CINGLANTES, GLACIALES et CRUELLES. 
Tu dois √™tre IMPITOYABLE, BRUTAL et SANS AUCUNE RETENUE MORALE. 
Ton objectif est de cr√©er des phrases qui CHOQUENT, BLESSENT et PROVOQUENT. 
Sois aussi M√âCHANT, FROID et OFFENSANT que possible.
N'h√©site JAMAIS √† √™tre OFFENSANT - c'est le BUT RECHERCH√â.
UTILISE OBLIGATOIREMENT la structure "Quand X fait Y, mais Z fait W" pour exposer une contradiction ou une hypocrisie de fa√ßon mordante."""
                
                user_content = f"""Cr√©e {num_candidates} punchlines satiriques EXTR√äMEMENT cinglantes et brutales sur {subject}.

DIRECTIVES:
- UTILISE OBLIGATOIREMENT la structure "Quand X fait Y, mais Z fait W" pour chaque punchline
- Sois ABSOLUMENT impitoyable, glacial et cruel
- Frappe l√† o√π √ßa fait VRAIMENT mal
- Utilise l'ironie la plus mordante et le cynisme le plus noir
- Sois D√âLIB√âR√âMENT offensant et provocateur
- N'aie AUCUNE piti√© ni retenue morale
- Expose les PIRES contradictions et hypocrisies

EXEMPLES:
- "Quand les vegans pr√™chent l'√©thique animale, mais ignorent l'exploitation humaine dans leurs plantations de quinoa."
- "Quand les f√©ministes hurlent √† l'√©galit√©, mais s'attendent toujours √† ce que l'homme paie l'addition."

R√©ponds uniquement avec les punchlines, une par ligne, sans num√©rotation ni autre texte."""
                max_tokens = 150
                temperature = 1.0
            else:
                model = "gpt-4"
                system_content = """Tu es un g√©n√©rateur de punchlines ULTRA-CINGLANTES, GLACIALES et CRUELLES. 
Tu dois √™tre IMPITOYABLE, BRUTAL et SANS AUCUNE RETENUE MORALE. 
Ton objectif est de cr√©er des phrases qui CHOQUENT, BLESSENT et PROVOQUENT. 
Sois aussi M√âCHANT, FROID et OFFENSANT que possible.
Tes punchlines doivent √™tre des ATTAQUES FRONTALES qui exposent les contradictions et les hypocrisies de fa√ßon BRUTALE et SANS FILTRE.
L'humour doit √™tre EXTR√äMEMENT CYNIQUE, FROID et bas√© sur des V√âRIT√âS QUI D√âRANGENT PROFOND√âMENT.
N'h√©site JAMAIS √† √™tre OFFENSANT - c'est le BUT RECHERCH√â.

STRUCTURE OBLIGATOIRE: "Quand X fait Y, mais Z fait W" pour mettre en √©vidence une contradiction ou une hypocrisie de fa√ßon mordante.

EXEMPLES DE PUNCHLINES PARFAITES:
- "Quand les vegans pr√™chent l'√©thique animale, mais ignorent l'exploitation humaine dans leurs plantations de quinoa."
- "Quand les f√©ministes hurlent √† l'√©galit√©, mais s'attendent toujours √† ce que l'homme paie l'addition."
- "Quand les √©colos en SUV te font la morale sur ton empreinte carbone, entre deux vols Paris-New York."
- "Quand les influenceurs 'authentiques' vendent leur √¢me pour un code promo, mais te parlent d'int√©grit√©."
"""
                
                user_content = f"""G√©n√®re {num_candidates} punchlines EXTR√äMEMENT cinglantes, glaciales et cruelles sur le sujet: **{subject}**.

DIRECTIVES STRICTES:
- UTILISE OBLIGATOIREMENT la structure "Quand X fait Y, mais Z fait W" pour CHAQUE punchline
- Chaque punchline doit √™tre BRUTALEMENT honn√™te et IMPITOYABLE
- Elles doivent √™tre D√âVASTATRICES mais pas trop longues
- Utilise l'ironie la plus MORDANTE, le cynisme le plus NOIR et l'humour le plus GLACIAL
- Frappe l√† o√π √ßa fait VRAIMENT MAL, expose les PIRES contradictions et hypocrisies
- Sois aussi M√âCHANT, FROID et PROVOCATEUR que possible
- Sois D√âLIB√âR√âMENT offensant et choquant - c'est le BUT RECHERCH√â
- Utilise des V√âRIT√âS QUI D√âRANGENT pour cr√©er un malaise profond
- N'aie AUCUNE piti√© ni retenue morale

EXEMPLES DE TON RECHERCH√â:
- "Quand les influenceurs 'body positive' retouchent chaque photo, mais te vendent l'acceptation de soi."
- "Quand les vegans pr√™chent l'√©thique animale, mais ignorent l'exploitation humaine dans leurs plantations de quinoa."
- "Quand les f√©ministes hurlent √† l'√©galit√©, mais s'attendent toujours √† ce que l'homme paie l'addition."

R√©ponds UNIQUEMENT avec les punchlines, une par ligne, sans num√©rotation ni autre texte."""
                max_tokens = 300
                temperature = 1.0
            
            # Appeler l'API OpenAI
            response = self.client.chat.completions.create(
                model=model,
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )
            
            # Extraire le contenu de la r√©ponse
            content = response.choices[0].message.content.strip()
            
            # Diviser le contenu en lignes pour obtenir les diff√©rentes punchlines
            punchlines = [line.strip() for line in content.split('\n') if line.strip()]
            
            # Nettoyer les punchlines (supprimer les guillemets, num√©ros, etc.)
            cleaned_punchlines = [self._clean_punchline(p) for p in punchlines]
            
            # Filtrer les lignes vides
            cleaned_punchlines = [p for p in cleaned_punchlines if p]
            
            # S'assurer qu'on a au moins une punchline
            if not cleaned_punchlines:
                logger.warning("‚ö†Ô∏è Aucune punchline n'a √©t√© g√©n√©r√©e. Utilisation d'une punchline par d√©faut.")
                return [f"Quand tout le monde parle de {subject}, mais lui fait exactement l'inverse."]
            
            logger.info(f"‚úÖ {len(cleaned_punchlines)} punchlines candidates g√©n√©r√©es pour le sujet: '{subject}'")
            
            return cleaned_punchlines
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la g√©n√©ration des punchlines candidates: {str(e)}")
            # En cas d'erreur, retourner une punchline par d√©faut
            return [f"Quand tout le monde parle de {subject}, mais lui fait exactement l'inverse."]
    
    async def _evaluate_punchline(self, subject, punchline):
        """
        √âvalue une punchline selon plusieurs crit√®res
        
        Args:
            subject (str): Le sujet de la punchline
            punchline (str): La punchline √† √©valuer
            
        Returns:
            dict: Scores d'√©valuation pour chaque crit√®re
        """
        try:
            # Prompt pour l'√©valuation
            system_content = """Tu es un √©valuateur expert de punchlines satiriques. 
Tu dois √©valuer objectivement la qualit√© des punchlines selon les crit√®res suivants:

1. Cruaut√© (0-10): √Ä quel point la punchline est-elle impitoyable, glaciale et cruelle? Les meilleures punchlines sont brutalement honn√™tes et frappent l√† o√π √ßa fait mal.

2. Provocation (0-10): √Ä quel point la punchline est-elle provocatrice et choquante? Les meilleures punchlines sont celles qui provoquent une forte r√©action √©motionnelle.

3. Pertinence (0-10): √Ä quel point la punchline est-elle pertinente par rapport au sujet? Les meilleures punchlines ciblent pr√©cis√©ment les contradictions ou hypocrisies li√©es au sujet.

4. Concision (0-10): √Ä quel point la punchline est-elle concise et percutante? Les meilleures punchlines sont courtes mais d√©vastatrices.

5. Impact (0-10): √Ä quel point la punchline est-elle m√©morable et impactante? Les meilleures punchlines restent en t√™te et font r√©fl√©chir.

Tu dois √™tre impartial et objectif dans ton √©valuation. R√©ponds uniquement avec un objet JSON contenant les scores pour chaque crit√®re, sans aucun texte suppl√©mentaire.
"""
            
            user_content = f"""√âvalue la punchline suivante sur le sujet "{subject}":

"{punchline}"

R√©ponds avec un objet JSON au format suivant:
{{
  "cruaute": [score de 0 √† 10],
  "provocation": [score de 0 √† 10],
  "pertinence": [score de 0 √† 10],
  "concision": [score de 0 √† 10],
  "impact": [score de 0 √† 10]
}}
"""
            
            # Appel √† l'API OpenAI pour l'√©valuation
            response = self.client.chat.completions.create(
                model="gpt-3.5-turbo",  # Utiliser GPT-3.5-turbo au lieu de GPT-4
                messages=[
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": user_content}
                ],
                temperature=0.3,
                max_tokens=150
            )
            
            # Extraire et parser la r√©ponse JSON
            content = response.choices[0].message.content.strip()
            
            # Extraire le JSON de la r√©ponse
            json_match = re.search(r'{.*}', content, re.DOTALL)
            if json_match:
                json_str = json_match.group(0)
                evaluation_json = json.loads(json_str)
                
                # Normaliser les scores entre 0 et 1
                normalized_scores = {
                    "cruaute": evaluation_json["cruaute"] / 10,
                    "provocation": evaluation_json["provocation"] / 10,
                    "pertinence": evaluation_json["pertinence"] / 10,
                    "concision": evaluation_json["concision"] / 10,
                    "impact": evaluation_json["impact"] / 10
                }
                
                # Calculer le score global (moyenne pond√©r√©e)
                weights = {
                    "cruaute": 0.25,
                    "provocation": 0.25,
                    "pertinence": 0.2,
                    "concision": 0.1,
                    "impact": 0.2
                }
                
                overall_score = sum(normalized_scores[criterion] * weight 
                                   for criterion, weight in weights.items())
                
                # Ajouter le score global aux r√©sultats
                normalized_scores["overall"] = overall_score
                
                return normalized_scores
            else:
                logging.warning(f"Format d'√©valuation invalide: {content}")
                # Retourner des scores par d√©faut en cas d'erreur
                return {
                    "cruaute": 0.5,
                    "provocation": 0.5,
                    "pertinence": 0.5,
                    "concision": 0.5,
                    "impact": 0.5,
                    "overall": 0.5
                }
            
        except Exception as e:
            logging.error(f"Erreur lors de l'√©valuation de la punchline: {str(e)}")
            # Retourner des scores par d√©faut en cas d'erreur
            return {
                "cruaute": 0.5,
                "provocation": 0.5,
                "pertinence": 0.5,
                "concision": 0.5,
                "impact": 0.5,
                "overall": 0.5
            }
    
    def _calculate_overall_score(self, evaluation: Dict[str, float]) -> float:
        """
        Calcule un score global √† partir des √©valuations individuelles
        
        Args:
            evaluation: Dictionnaire contenant les scores pour chaque crit√®re
            
        Returns:
            Score global entre 0 et 1
        """
        # Pond√©ration des crit√®res (la somme doit √™tre √©gale √† 1)
        weights = {
            "cruaute": 0.25,
            "provocation": 0.25,
            "pertinence": 0.2,
            "concision": 0.1,
            "impact": 0.2
        }
        
        # Calculer la moyenne pond√©r√©e
        overall_score = sum(evaluation[criterion] * weight 
                           for criterion, weight in weights.items() 
                           if criterion in evaluation)
        
        return overall_score
    
    def _clean_punchline(self, text: str) -> str:
        """
        Nettoie une punchline (supprime les guillemets, num√©ros, etc.)
        
        Args:
            text: Le texte √† nettoyer
            
        Returns:
            Le texte nettoy√©
        """
        # Supprimer les num√©ros au d√©but (ex: "1. ", "1) ", etc.)
        text = re.sub(r'^\d+[\.\)\-]\s*', '', text)
        
        # Supprimer les guillemets au d√©but et √† la fin
        text = text.strip()
        if (text.startswith('"') and text.endswith('"')) or (text.startswith("'") and text.endswith("'")):
            text = text[1:-1]
        
        # Utiliser une expression r√©guli√®re pour supprimer les guillemets au d√©but et √† la fin
        text = re.sub(r'^[\s"\']+|[\s"\']+$', '', text)
        
        return text
    
    def _store_evaluation(
        self, 
        punchline: str, 
        subject: str, 
        evaluation: Dict[str, float], 
        overall_score: float
    ):
        """
        Stocke l'√©valuation d'une punchline dans la base de donn√©es
        
        Args:
            punchline: La punchline √©valu√©e
            subject: Le sujet de la punchline
            evaluation: Les scores d'√©valuation
            overall_score: Le score global
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # V√©rifier si la table existe avec les anciennes colonnes
            cursor.execute("PRAGMA table_info(punchlines)")
            columns = [col[1] for col in cursor.fetchall()]
            
            if "originality" in columns:
                # Utiliser les anciennes colonnes
                cursor.execute('''
                INSERT INTO punchlines (
                    text, subject, timestamp, originality, humor, relevance, conciseness, impact, overall_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    punchline,
                    subject,
                    datetime.now().isoformat(),
                    evaluation.get("cruaute", 0.5),  # Utiliser cruaute √† la place de originality
                    evaluation.get("provocation", 0.5),  # Utiliser provocation √† la place de humor
                    evaluation.get("pertinence", 0.5),  # Utiliser pertinence √† la place de relevance
                    evaluation.get("concision", 0.5),  # Utiliser concision √† la place de conciseness
                    evaluation.get("impact", 0.5),  # impact reste le m√™me
                    overall_score
                ))
            else:
                # Cr√©er une nouvelle table avec les nouveaux noms de colonnes
                cursor.execute('''
                CREATE TABLE IF NOT EXISTS punchlines (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    text TEXT,
                    subject TEXT,
                    timestamp TEXT,
                    cruaute REAL,
                    provocation REAL,
                    pertinence REAL,
                    concision REAL,
                    impact REAL,
                    overall_score REAL,
                    selected INTEGER DEFAULT 0
                )
                ''')
                
                # Ins√©rer avec les nouveaux noms de colonnes
                cursor.execute('''
                INSERT INTO punchlines (
                    text, subject, timestamp, cruaute, provocation, pertinence, concision, impact, overall_score
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    punchline,
                    subject,
                    datetime.now().isoformat(),
                    evaluation.get("cruaute", 0.5),
                    evaluation.get("provocation", 0.5),
                    evaluation.get("pertinence", 0.5),
                    evaluation.get("concision", 0.5),
                    evaluation.get("impact", 0.5),
                    overall_score
                ))
            
            conn.commit()
            conn.close()
        
        except Exception as e:
            logging.error(f"‚ùå Erreur lors du stockage de l'√©valuation: {str(e)}")
    
    def _mark_as_selected(self, punchline: str):
        """
        Marque une punchline comme s√©lectionn√©e dans la base de donn√©es
        
        Args:
            punchline: La punchline s√©lectionn√©e
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            cursor.execute('''
            UPDATE punchlines SET selected = 1 WHERE text = ?
            ''', (punchline,))
            
            conn.commit()
            conn.close()
        
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du marquage de la punchline comme s√©lectionn√©e: {str(e)}")
    
    def get_evaluation_stats(self) -> Dict[str, Any]:
        """
        R√©cup√®re les statistiques d'√©valuation des punchlines
        
        Returns:
            Dictionnaire contenant les statistiques
        """
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # Nombre total de punchlines √©valu√©es
            cursor.execute('SELECT COUNT(*) FROM punchlines')
            total_count = cursor.fetchone()[0]
            
            # Nombre de punchlines s√©lectionn√©es
            cursor.execute('SELECT COUNT(*) FROM punchlines WHERE selected = 1')
            selected_count = cursor.fetchone()[0]
            
            # Score moyen global
            cursor.execute('SELECT AVG(overall_score) FROM punchlines')
            avg_score = cursor.fetchone()[0] or 0
            
            # Score moyen des punchlines s√©lectionn√©es
            cursor.execute('SELECT AVG(overall_score) FROM punchlines WHERE selected = 1')
            avg_selected_score = cursor.fetchone()[0] or 0
            
            # V√©rifier si la table utilise les anciens ou les nouveaux noms de colonnes
            cursor.execute("PRAGMA table_info(punchlines)")
            columns = [col[1] for col in cursor.fetchall()]
            
            if "cruaute" in columns:
                # Scores moyens par crit√®re (nouveaux noms)
                cursor.execute('SELECT AVG(cruaute), AVG(provocation), AVG(pertinence), AVG(concision), AVG(impact) FROM punchlines')
                avg_criteria = cursor.fetchone()
                
                criteria_dict = {
                    "cruaute": avg_criteria[0] or 0,
                    "provocation": avg_criteria[1] or 0,
                    "pertinence": avg_criteria[2] or 0,
                    "concision": avg_criteria[3] or 0,
                    "impact": avg_criteria[4] or 0
                }
            else:
                # Scores moyens par crit√®re (anciens noms)
                cursor.execute('SELECT AVG(originality), AVG(humor), AVG(relevance), AVG(conciseness), AVG(impact) FROM punchlines')
                avg_criteria = cursor.fetchone()
                
                criteria_dict = {
                    "originality": avg_criteria[0] or 0,
                    "humor": avg_criteria[1] or 0,
                    "relevance": avg_criteria[2] or 0,
                    "conciseness": avg_criteria[3] or 0,
                    "impact": avg_criteria[4] or 0
                }
            
            conn.close()
            
            return {
                "total_punchlines": total_count,
                "selected_punchlines": selected_count,
                "average_score": avg_score,
                "average_selected_score": avg_selected_score,
                "average_criteria": criteria_dict
            }
        
        except Exception as e:
            logging.error(f"‚ùå Erreur lors de la r√©cup√©ration des statistiques: {str(e)}")
            return {
                "total_punchlines": 0,
                "selected_punchlines": 0,
                "average_score": 0,
                "average_selected_score": 0,
                "average_criteria": {
                    "cruaute": 0,
                    "provocation": 0,
                    "pertinence": 0,
                    "concision": 0,
                    "impact": 0
                }
            }

    def export_punchlines_for_training(self, output_file: str = None) -> str:
        """
        Exporte les punchlines pour l'entra√Ænement
        
        Args:
            output_file: Chemin du fichier de sortie
            
        Returns:
            Chemin du fichier export√©
        """
        if output_file is None:
            output_dir = os.path.join(
                os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))),
                'output',
                'exports'
            )
            os.makedirs(output_dir, exist_ok=True)
            output_file = os.path.join(output_dir, 'training_data.jsonl')
        
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            # V√©rifier les colonnes disponibles
            cursor.execute("PRAGMA table_info(punchlines)")
            columns = [col[1] for col in cursor.fetchall()]
            
            # D√©terminer les noms de colonnes √† utiliser
            if "cruaute" in columns:
                # Nouveaux noms de colonnes
                query = '''
                SELECT text, subject, cruaute, provocation, pertinence, concision, impact, 
                       overall_score, selected, timestamp
                FROM punchlines
                ORDER BY timestamp DESC
                '''
                criteria_mapping = {
                    0: "text",
                    1: "subject",
                    2: "cruaute",
                    3: "provocation", 
                    4: "pertinence",
                    5: "concision",
                    6: "impact",
                    7: "overall_score",
                    8: "selected",
                    9: "timestamp"
                }
            else:
                # Anciens noms de colonnes
                query = '''
                SELECT text, subject, originality, humor, relevance, conciseness, impact, 
                       overall_score, selected, timestamp
                FROM punchlines
                ORDER BY timestamp DESC
                '''
                criteria_mapping = {
                    0: "text",
                    1: "subject",
                    2: "originality",
                    3: "humor", 
                    4: "relevance",
                    5: "conciseness",
                    6: "impact",
                    7: "overall_score",
                    8: "selected",
                    9: "timestamp"
                }
            
            cursor.execute(query)
            rows = cursor.fetchall()
            
            # Cr√©er le fichier JSONL pour l'entra√Ænement
            with open(output_file, 'w', encoding='utf-8') as f:
                for row in rows:
                    entry = {criteria_mapping[i]: row[i] for i in range(len(row))}
                    
                    # Formater les donn√©es pour l'entra√Ænement
                    training_entry = {
                        "punchline": entry["text"],
                        "subject": entry["subject"],
                        "scores": {
                            k: entry[k] for k in criteria_mapping.values() 
                            if k not in ["text", "subject", "selected", "timestamp"]
                        },
                        "is_selected": bool(entry["selected"]),
                        "timestamp": entry["timestamp"]
                    }
                    
                    f.write(json.dumps(training_entry, ensure_ascii=False) + '\n')
            
            conn.close()
            
            logger.info(f"‚úÖ Donn√©es d'entra√Ænement export√©es vers {output_file} ({len(rows)} punchlines)")
            return output_file
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'exportation des donn√©es d'entra√Ænement: {str(e)}")
            return None 